---
title: "goodwin_mwe"
output: html_document
date: "2025-08-17"
editor_options: 
  markdown: 
    wrap: 72
---
# Setup 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lme4)
library(lmerTest)
library(ggrepel)
```

The data in this project are corpus examples of dative verbs. Dative
verbs take two arguments, and often allow two orders for the arguments:

(1) The man threw her a ball (DO form)
(2) The man threw a ball to her (PP form)

Speakers have preferences for which form/ordering is better, which
depend on some generally-applicable probabilistic constraints (e.g. short nousn are preferred earlier), as well as
experience with the verb in either the DO or PP form.

This dataset is about two questions:

A. How does verb frequency influences the strength of its ordering
preference (are more frequent verbs more likely to have strong
preferences for one form or the other? Is this a gradient relationship,
is it linearly increasing or with a step function?)

B. Does exposure to "non-dative uses" influence dative ordering
preferences? Non-dative uses are sentences that have PP or DO surface
syntax, but due to semantic constraints do not alternate:

(3) The man threw a ball to the floor / \* Threw the floor a ball
(4) \* It took five seconds to the man/ It took the man five seconds

Because these examples do not alternate, speakers might not use them as
evidence of a verb preferring a certain form. On the other hand, certain
verbs are more frequent in non-dative than dative uses; for such verbs,
the speaker may have mostly non-dative evidence to draw on.

```{r}
# This is the gold labels- includes raw examples and their coding 
df.allLabels <- read_tsv('gold_labels.tsv')

# This is a dataframe with frequency estimates for each verb 
# As well as their estimated preferences for DO or PP form, in both datives-only ("imputed") or datives and non-datives ("surface")
df.frequencyEstimates <- read_tsv('frequency_estimates.tsv')
```

The **theme** refers to the argument being transferred, and the
**recipient** to where the theme ends up. E.g. "give a dog a bone" and
"give a bone to a dog" both have recipient 'a dog' and theme 'a bone'.

The dependent variable: 
-   structure (DO or PP)

The fixed effects represent generally applicable probabilistic constraints on ordering, and are listed below. 
The details are not super important for this analysis, they have been
identified/reported/tested elsewhere before (e.g. Bresnan et al., 2006) but here they are: 

-   Verb sense (Abstract, Transfer of Possession, Future Possession... )
-   Theme & Recipient Pronominality (fullNP or pronominal)
-   Theme & Recipient Definiteness (definite or indefinite)
-   Theme & Recipient Number (singular/plural/NA)
-   Theme & Recipient Given (Recently-mentioned or New )
-   Theme Concrete (concrete or Not Concrete)
-   Recipient Animate (animate or inanimate)
-   Recipient Person (local (1st or 2nd person) or nonlocal (3rd
    person))
-   Previous Structure (recent discourse has DO or PP-like sentence)
-   Length Difference (sign-preserving log of the difference in number
    of graphemic words between theme and recipient)

The random by-verb intercept represents the verb-specific knowledge speakers 
have for whether a verb prefers the DO or PP form. There is one for each verb
lemma ('give', 'send', etc).

# Descriptive plots 

Verbs vary widely in their preference for a form (DO or PP) and their frequency 
in dative uses. 
More frequent verbs prefer the DO form (positive trend in this graph) but a 
larger number of verb lemmas prefer the PP form (more mass below .5): 
```{r}

df.frequencyEstimates %>% 
  ggplot(aes(x = log(imputed_dative_frequency), 
             y = imputed_DO_skew)) + 
  geom_point() +
  geom_text_repel(aes(label = verbLemma)) + 
  labs(title = "Verb Preference For DO Form by Frequency")
```

Some verbs have a very similar DO/PP preference between their dative and non-dative
uses (e.g. "promise", "allot"), some do not ("shoot", "vote", "grant"). 
This plot shows that, for at least some verbs, a speakers' experience with the verb 
preferring the DO or PP for really depends on whether you count non-dative uses. 
```{r}

df.frequencyEstimates %>% 
  ggplot(aes(x = nonDative_DO_skew, 
             y = imputed_DO_skew, 
             alpha = log(surface_ditransitive_frequency))) + 
  geom_abline(slope = 1,intercept = 0, linetype = 2)+ 
  geom_point() +
  geom_text_repel(aes(label = verbLemma)) + 
  theme(legend.position = "none") + 
  labs(title = "DO Preference Amongst Dative and Non-Dative Uses", 
       subtitle = "Darker verbs are more frequent (log scaled)") 
```


# Prepare Model Data, Code Variables
```{r}
# We fit the model on 6837 examples of 91 verb lemmas 
## (those w at least 10 true dative examples) 
# Sentences that had some issue with coding, but were definitely dative, are excluded
## from the model but are otherwise considered dative for estimating frequency values 
df.datives <- df.allLabels %>% 
  filter(isDative == TRUE & ableToTiebreak == TRUE & 
           themeDefinite != "UNK" & recipientDefinite != "UNK") %>% 
  group_by(verbLemma) %>% 
  add_count() %>% 
  filter(n >=10) %>% 
  select(-n) %>% 
  ungroup()

df.datives %>% nrow()
df.datives %>% group_by(verbLemma) %>% summarize() %>% nrow()

df.datives %>% group_by(verbLemma) %>% add_count() %>% filter(n>=50) %>% summarize() %>% nrow()

# We fit the model using the OG Bresnan et al animacy scheme, with 2 labels:
## RecipientAnimacy: 1 (human + animal) or 0 
## ThemeConcrete: 1 (for concrete things) or 0 
df.datives_recoded <- df.datives %>% 
  mutate(
    themeConcrete = case_when(
      themeAnimacy %in% c('Human', 'Animal', 'Concrete', 'Vehicle', 'Machine') ~ 1,
      .default = 0), # Org, NotConc, time, Mix, place, no way to decide, machine
    themeAnimacy = case_when(
      themeAnimacy %in% c('Human', 'Animal') ~ 1, .default = 0),
    recipientConcrete = case_when(
      recipientAnimacy %in% c('Human', 'Animal', 'Concrete', 'Vehicle', 'Machine') ~ 1,
      .default = 0),
    recipientAnimacy = case_when(
      recipientAnimacy %in% c('Human', 'Animal') ~ 1, .default = 0))


# Recode all the fixed effects
df.model_data_corpus <- df.datives_recoded %>% 
  mutate(
    structure = case_when(
      structure == "PP" ~ 0, 
      structure == "DO" ~ 1),
    themeNumber = case_when(
      is.na(themeNumber) ~ "NA", 
      .default = themeNumber),
    recipientNumber = case_when(
      is.na(recipientNumber)~"NA", 
      .default = recipientNumber),
    recipientPerson = case_when(
      recipientPerson == 'third' ~ 'nonlocal', 
      .default = 'Local'),
    themePerson = case_when(
      themePerson == 'third' ~ 'nonlocal', 
      .default = 'Local'))%>% 
  mutate(
    previousStructure = factor(previousStructure, levels = c("None", "DO", "PP")),
    themeNumber = as.factor(themeNumber), 
    recipientNumber = as.factor(recipientNumber),
    themeDefinite = as.factor(themeDefinite),
    recipientDefinite = as.factor(recipientDefinite),
    themeGiven = as.factor(themeGiven), 
    recipientGiven = as.factor(recipientGiven), 
    themeAnimacy = as.factor(themeAnimacy),
    themeConcrete = as.factor(themeConcrete),
    recipientAnimacy = as.factor(recipientAnimacy), 
    themePronominal = as.factor(themePronominal),
    recipientPronominal = as.factor(recipientPronominal),
    themePerson = as.factor(themePerson), 
    recipientPerson = as.factor(recipientPerson),
    verbSense = as.factor(verbSense), 
    lengthDifference = case_when(
      themeLength > recipientLength ~ log(abs(themeLength-recipientLength) +.5),
      .default = -1*log(abs(themeLength-recipientLength) +.5))
  )

  
```

# Fit the model 
This model does not converge: 
```{r}

fit.corpusModel <- glmer(structure ~
                       verbSense +
                       themeGiven + recipientGiven +
                       themePronominal + recipientPronominal +
                       themeDefinite + recipientDefinite +
                       recipientAnimacy +
                       themeConcrete +
                       recipientPerson +
                       recipientNumber + themeNumber +
                       previousStructure +
                       lengthDifference +
                       (1|verbLemma),
                     data = df.model_data_corpus,
                     family = "binomial")

fit.corpusModel %>% summary()
```
**Question 1: Do more frequent verbs have ordering preferences that are driven more by item-specific knowledge?** 

The following plot shows that the verbs' random intercepts (representing speakers' 
verb-specific knowledge, after abstracting away generally-applicable rules) 
increase in magnitude with log verb frequency. Since more frequent verbs have 
larger intercepts, we take this to mean that speakers rely more on item-specific 
knowledge for those verbs. 

```{r}


df.randomEffects <- ranef(fit.corpusModel) %>% 
  as_tibble() %>%
  mutate(verbLemma = grp, 
          estimateIntercept = condval, 
          estimateSD = condsd) %>% 
  select(verbLemma, estimateIntercept, estimateSD)

df.randomEffects %>% 
  left_join(df.frequencyEstimates) %>% 
  ggplot(aes(x = log(surface_ditransitive_frequency), 
             y = abs(estimateIntercept))) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_text_repel(aes(label = verbLemma))
```

**Question 2: Does experience with non-dative verbs influence ordering preferences?**

Recall that our second question was whether speakers base their ordering 
preferences on experience with a verb in non-dative structues. 

In the following model, the proportion of DO forms amongst non-dative uses was 
not found to be a significant influence on the verb-specific intercept; we therefore
cannot conclude that additional evidence with a verb in non-dative structures is 
"useful" for speakers when deciding how to order datives. 
```{r}
df.experienceModelData <- ranef(fit.corpusModel) %>% 
  as_tibble() %>%
  mutate(verbLemma = grp, 
          estimateIntercept = condval, 
          estimateSD = condsd) %>% 
  select(verbLemma, estimateIntercept, estimateSD) %>% 
  left_join(df.frequencyEstimates)

fit.experienceModel <- lm(estimateIntercept ~ nonDative_DO_skew + imputed_DO_skew, 
                          data = df.experienceModelData)

fit.experienceModel %>%  summary()
```

# Questions 
(1) Model 0 (fit.corpusModel) obviously doesn't converge. 
So far I have fit & reported a BRMS version of this model, but I want to really understand what it actually means that a model doesn't convergem and in some sense how it is "ok" to then just turn around and fit the same model with a Bayesian approach (although maybe that is a question for the other track)

(2) I think I still don't understand how to interpret individual things, for example the random effects- what space are they in? I know that if they are positive that means that the verb prefer the DO form (because of how I coded the response variable) but I actually also don't **really** understand the coding schemes and the terms applied 

(3) Just some general understanding where I feel so behind...for example, what are families. what is a "link" function. I know its logistic (dep variable binary) so I type in a family = binomial and I guess that makes sense because I know that binomial function represents a number of successes (from a paradigm with two outcomes) but how is that function used by this model?

(4) I frequently feel my analyses notebooks get messy/difficult to follow very, very quickly. The internet has a lot of advice like "make sure your filepaths are relative/portable to someone who clones your repository". This is good practice but I do think there is some bigger problem about structure/organization that I find hard to articulate. Happy to hear any advice. 

